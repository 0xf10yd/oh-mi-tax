{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc363f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "736a3b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = 'Birch_10.xlsx'\n",
    "df = pd.read_excel(fileName, index_col=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecf29af0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n11902</th>\n",
       "      <th>prep</th>\n",
       "      <th>n1</th>\n",
       "      <th>n00200</th>\n",
       "      <th>a18300</th>\n",
       "      <th>a19300</th>\n",
       "      <th>a18425</th>\n",
       "      <th>n07220</th>\n",
       "      <th>a07220</th>\n",
       "      <th>n03300</th>\n",
       "      <th>...</th>\n",
       "      <th>n02300</th>\n",
       "      <th>a04800</th>\n",
       "      <th>n07260</th>\n",
       "      <th>a59720</th>\n",
       "      <th>a11070</th>\n",
       "      <th>n11070</th>\n",
       "      <th>a07180</th>\n",
       "      <th>n59720</th>\n",
       "      <th>n00900</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.656844</td>\n",
       "      <td>-0.725012</td>\n",
       "      <td>-0.679983</td>\n",
       "      <td>-0.707169</td>\n",
       "      <td>-0.601479</td>\n",
       "      <td>-0.539202</td>\n",
       "      <td>-0.441651</td>\n",
       "      <td>-0.767169</td>\n",
       "      <td>-0.73214</td>\n",
       "      <td>-0.239800</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.768795</td>\n",
       "      <td>-0.715549</td>\n",
       "      <td>-0.694939</td>\n",
       "      <td>-0.426612</td>\n",
       "      <td>-0.504469</td>\n",
       "      <td>-0.513154</td>\n",
       "      <td>-0.654800</td>\n",
       "      <td>-0.453259</td>\n",
       "      <td>-0.828691</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.757440</td>\n",
       "      <td>-0.860670</td>\n",
       "      <td>-0.788663</td>\n",
       "      <td>-0.808400</td>\n",
       "      <td>-0.646187</td>\n",
       "      <td>-0.689814</td>\n",
       "      <td>-0.522629</td>\n",
       "      <td>-0.767169</td>\n",
       "      <td>-0.73214</td>\n",
       "      <td>-0.239800</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.768795</td>\n",
       "      <td>-0.785157</td>\n",
       "      <td>-0.694939</td>\n",
       "      <td>-0.426612</td>\n",
       "      <td>-0.504469</td>\n",
       "      <td>-0.513154</td>\n",
       "      <td>-0.654800</td>\n",
       "      <td>-0.453259</td>\n",
       "      <td>-0.828691</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.712397</td>\n",
       "      <td>-0.860670</td>\n",
       "      <td>-0.753251</td>\n",
       "      <td>-0.769116</td>\n",
       "      <td>-0.646187</td>\n",
       "      <td>-0.689814</td>\n",
       "      <td>-0.522629</td>\n",
       "      <td>-0.767169</td>\n",
       "      <td>-0.73214</td>\n",
       "      <td>-0.239800</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.768795</td>\n",
       "      <td>-0.760283</td>\n",
       "      <td>-0.694939</td>\n",
       "      <td>-0.426612</td>\n",
       "      <td>-0.504469</td>\n",
       "      <td>-0.513154</td>\n",
       "      <td>-0.654800</td>\n",
       "      <td>-0.453259</td>\n",
       "      <td>-0.828691</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.661348</td>\n",
       "      <td>-0.493158</td>\n",
       "      <td>-0.570082</td>\n",
       "      <td>-0.584786</td>\n",
       "      <td>1.947344</td>\n",
       "      <td>1.012044</td>\n",
       "      <td>2.728755</td>\n",
       "      <td>-0.767169</td>\n",
       "      <td>-0.73214</td>\n",
       "      <td>-0.239800</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.768795</td>\n",
       "      <td>1.433423</td>\n",
       "      <td>-0.694939</td>\n",
       "      <td>-0.426612</td>\n",
       "      <td>-0.504470</td>\n",
       "      <td>-0.513154</td>\n",
       "      <td>-0.654800</td>\n",
       "      <td>-0.453259</td>\n",
       "      <td>-0.396383</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.427125</td>\n",
       "      <td>-0.490692</td>\n",
       "      <td>-0.471171</td>\n",
       "      <td>-0.521327</td>\n",
       "      <td>-0.614600</td>\n",
       "      <td>-0.689814</td>\n",
       "      <td>-0.522629</td>\n",
       "      <td>-0.767169</td>\n",
       "      <td>-0.73214</td>\n",
       "      <td>-0.239815</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.323192</td>\n",
       "      <td>-0.766942</td>\n",
       "      <td>-0.694943</td>\n",
       "      <td>0.151573</td>\n",
       "      <td>-0.055994</td>\n",
       "      <td>-0.072119</td>\n",
       "      <td>-0.654804</td>\n",
       "      <td>0.137692</td>\n",
       "      <td>-0.387560</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     n11902      prep        n1    n00200    a18300    a19300    a18425  \\\n",
       "0 -0.656844 -0.725012 -0.679983 -0.707169 -0.601479 -0.539202 -0.441651   \n",
       "1 -0.757440 -0.860670 -0.788663 -0.808400 -0.646187 -0.689814 -0.522629   \n",
       "2 -0.712397 -0.860670 -0.753251 -0.769116 -0.646187 -0.689814 -0.522629   \n",
       "3 -0.661348 -0.493158 -0.570082 -0.584786  1.947344  1.012044  2.728755   \n",
       "4 -0.427125 -0.490692 -0.471171 -0.521327 -0.614600 -0.689814 -0.522629   \n",
       "\n",
       "     n07220   a07220    n03300  ...    n02300    a04800    n07260    a59720  \\\n",
       "0 -0.767169 -0.73214 -0.239800  ... -0.768795 -0.715549 -0.694939 -0.426612   \n",
       "1 -0.767169 -0.73214 -0.239800  ... -0.768795 -0.785157 -0.694939 -0.426612   \n",
       "2 -0.767169 -0.73214 -0.239800  ... -0.768795 -0.760283 -0.694939 -0.426612   \n",
       "3 -0.767169 -0.73214 -0.239800  ... -0.768795  1.433423 -0.694939 -0.426612   \n",
       "4 -0.767169 -0.73214 -0.239815  ... -0.323192 -0.766942 -0.694943  0.151573   \n",
       "\n",
       "     a11070    n11070    a07180    n59720    n00900  label  \n",
       "0 -0.504469 -0.513154 -0.654800 -0.453259 -0.828691      1  \n",
       "1 -0.504469 -0.513154 -0.654800 -0.453259 -0.828691      3  \n",
       "2 -0.504469 -0.513154 -0.654800 -0.453259 -0.828691      1  \n",
       "3 -0.504470 -0.513154 -0.654800 -0.453259 -0.396383      3  \n",
       "4 -0.055994 -0.072119 -0.654804  0.137692 -0.387560      5  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba9e6835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n11902</th>\n",
       "      <th>prep</th>\n",
       "      <th>n1</th>\n",
       "      <th>n00200</th>\n",
       "      <th>a18300</th>\n",
       "      <th>a19300</th>\n",
       "      <th>a18425</th>\n",
       "      <th>n07220</th>\n",
       "      <th>a07220</th>\n",
       "      <th>n03300</th>\n",
       "      <th>...</th>\n",
       "      <th>a00600</th>\n",
       "      <th>n02300</th>\n",
       "      <th>a04800</th>\n",
       "      <th>n07260</th>\n",
       "      <th>a59720</th>\n",
       "      <th>a11070</th>\n",
       "      <th>n11070</th>\n",
       "      <th>a07180</th>\n",
       "      <th>n59720</th>\n",
       "      <th>n00900</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25000.0</td>\n",
       "      <td>362000.0</td>\n",
       "      <td>695422.0</td>\n",
       "      <td>500000.0</td>\n",
       "      <td>35652.0</td>\n",
       "      <td>12500.0</td>\n",
       "      <td>45000.0</td>\n",
       "      <td>16341.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3231.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>90457.0</td>\n",
       "      <td>3422.0</td>\n",
       "      <td>65142.0</td>\n",
       "      <td>23652.0</td>\n",
       "      <td>1295.0</td>\n",
       "      <td>3411.0</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>86000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>356348.0</td>\n",
       "      <td>32444.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>96334.0</td>\n",
       "      <td>988634.0</td>\n",
       "      <td>3621.0</td>\n",
       "      <td>36987.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>...</td>\n",
       "      <td>66489.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>264312.0</td>\n",
       "      <td>6699.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>63212.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>6984.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>263211.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36.0</td>\n",
       "      <td>104556.0</td>\n",
       "      <td>251122.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>2654.0</td>\n",
       "      <td>98977.0</td>\n",
       "      <td>321424.0</td>\n",
       "      <td>639.0</td>\n",
       "      <td>98744.0</td>\n",
       "      <td>2110.0</td>\n",
       "      <td>...</td>\n",
       "      <td>49878.0</td>\n",
       "      <td>6633.0</td>\n",
       "      <td>350000.0</td>\n",
       "      <td>9778.0</td>\n",
       "      <td>98788.0</td>\n",
       "      <td>97888.0</td>\n",
       "      <td>2559.0</td>\n",
       "      <td>9878.0</td>\n",
       "      <td>989.0</td>\n",
       "      <td>34555.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>147.0</td>\n",
       "      <td>200154.0</td>\n",
       "      <td>390142.0</td>\n",
       "      <td>4556.0</td>\n",
       "      <td>498777.0</td>\n",
       "      <td>24556.0</td>\n",
       "      <td>459877.0</td>\n",
       "      <td>2344.0</td>\n",
       "      <td>3411.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7054.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>804563.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>314.0</td>\n",
       "      <td>69740.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>1578.0</td>\n",
       "      <td>334.0</td>\n",
       "      <td>78997.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>883460.0</td>\n",
       "      <td>493370.0</td>\n",
       "      <td>1075170.0</td>\n",
       "      <td>85960.0</td>\n",
       "      <td>172842.0</td>\n",
       "      <td>13447.0</td>\n",
       "      <td>17786.0</td>\n",
       "      <td>43670.0</td>\n",
       "      <td>1404.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>...</td>\n",
       "      <td>165759.0</td>\n",
       "      <td>4590.0</td>\n",
       "      <td>282319.0</td>\n",
       "      <td>1390.0</td>\n",
       "      <td>996893.0</td>\n",
       "      <td>375240.0</td>\n",
       "      <td>2760.0</td>\n",
       "      <td>2320.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7450.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     n11902      prep         n1    n00200    a18300   a19300    a18425  \\\n",
       "0   25000.0  362000.0   695422.0  500000.0   35652.0  12500.0   45000.0   \n",
       "1       0.0   15000.0   356348.0   32444.0     264.0  96334.0  988634.0   \n",
       "2      36.0  104556.0   251122.0      32.0    2654.0  98977.0  321424.0   \n",
       "3     147.0  200154.0   390142.0    4556.0  498777.0  24556.0  459877.0   \n",
       "4  883460.0  493370.0  1075170.0   85960.0  172842.0  13447.0   17786.0   \n",
       "\n",
       "    n07220   a07220  n03300  ...    a00600  n02300    a04800  n07260  \\\n",
       "0  16341.0  10000.0    10.0  ...    3231.0  5000.0   90457.0  3422.0   \n",
       "1   3621.0  36987.0    36.0  ...   66489.0     0.0  264312.0  6699.0   \n",
       "2    639.0  98744.0  2110.0  ...   49878.0  6633.0  350000.0  9778.0   \n",
       "3   2344.0   3411.0   145.0  ...    7054.0  1500.0  804563.0    63.0   \n",
       "4  43670.0   1404.0   190.0  ...  165759.0  4590.0  282319.0  1390.0   \n",
       "\n",
       "     a59720    a11070  n11070  a07180  n59720    n00900  \n",
       "0   65142.0   23652.0  1295.0  3411.0  1240.0   86000.0  \n",
       "1     322.0   63212.0    32.0  6984.0  3600.0  263211.0  \n",
       "2   98788.0   97888.0  2559.0  9878.0   989.0   34555.0  \n",
       "3     314.0   69740.0   245.0  1578.0   334.0   78997.0  \n",
       "4  996893.0  375240.0  2760.0  2320.0     0.0    7450.0  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_excel('samples.xlsx', index_col=0)\n",
    "df2 = df2.astype(float)\n",
    "df2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e81627a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.55899168e+01  8.69646639e+02  8.23996615e+02  7.34895173e+02\n",
      "  1.78305788e+01  6.93731571e+00  3.90200723e+01  1.13263539e+02\n",
      "  5.16944438e+01  1.51486819e+00  8.32402106e+01  6.41526095e+01\n",
      "  7.05498556e+00 -6.54971448e-01  6.50050507e-01 -2.22583106e-01\n",
      "  1.57731190e+02 -4.70610042e-01  5.21102021e-01  1.81326297e+00\n",
      "  5.00295271e+01  3.90730731e+01  2.40507378e+01  2.94146030e+01\n",
      "  7.51310789e+01  7.23148002e+01  1.46497975e+00  1.99798654e+01\n",
      "  2.21164322e+01  2.05427650e+01  2.47080229e+02 -1.79264088e-01\n",
      "  4.16904557e+00  7.12141101e+01  3.03394601e+00  1.35133376e+02\n",
      "  1.46011175e+02  1.22828787e+02  8.55014895e+00  1.67363954e+02\n",
      "  5.87587168e+00  7.41216205e+02]\n"
     ]
    }
   ],
   "source": [
    "asd = {'prep': [415.8590178767395, 349.6027212276311],\n",
    " 'a03300': [89.59089907660008, 17.93750079048541],\n",
    " 'a02500': [1427.0222928053736, 1035.5710214079659],\n",
    " 'schf': [15.802995536864515, 9.9262975060373],\n",
    " 'a18450': [97.38980281439635, 49.59050315715301],\n",
    " 'a07260': [15.247126984693264, 8.247893442524319],\n",
    " 'a04800': [23657.302814791445, 18682.020547798114],\n",
    " 'a11070': [191.7805648333245, 95.82591988932174],\n",
    " 'a11901': [449.1750409189924, 294.1968267914187],\n",
    " 'a00650': [491.98878198236747, 279.24639697892223],\n",
    " 'a06500': [3972.5981478650974, 2687.3023392670675],\n",
    " 'n00200': [679.6385048079165, 536.943390582368],\n",
    " 'a07180': [20.30224163897573, 13.136571545533544],\n",
    " 'a10300': [4122.705347085954, 2881.6991511924753],\n",
    " 'n59660': [224.9802106283514, 103.75150007343427],\n",
    " 'a59720': [444.8534469603124, 188.4256450403093],\n",
    " 'n11070': [142.97345362232386, 72.55567577675733],\n",
    " 'n02300': [69.47594955118288, 52.33208026629609],\n",
    " 'n03300': [5.7627198046544, 1.2702390777992443],\n",
    " 'n18425': [211.30360211117664, 140.6222333567569],\n",
    " 'a18425': [1137.8742164756889, 600.0658079333157],\n",
    " 'a19300': [1639.3163824902158, 1127.5447109206677],\n",
    " 'n07260': [25.194632883454926, 17.36420941435113],\n",
    " 'a07220': [190.77796680042832, 137.83911773761324],\n",
    " 'a18300': [1928.9756151803754, 1257.24829015717],\n",
    " 'a00300': [357.21509350692486, 265.34039427790213],\n",
    " 'n09600': [20.57796989059724, 4.5803084546771275],\n",
    " 'a09600': [115.87840045863962, 20.772835727578705],\n",
    " 'a19700': [709.8922746999984, 484.42517732165624],\n",
    " 'a00900': [1241.0580543651497, 943.8872366013386],\n",
    " 'n11902': [688.1323594104123, 509.4265871893509],\n",
    " 'a00600': [676.7585632329103, 409.56271215655823],\n",
    " 'a59660': [502.56257208511806, 217.32144905244292],\n",
    " 'n1': [843.1744784605747, 649.0837014307831],\n",
    " 'n07220': [143.31578518142132, 108.54692861306756],\n",
    " 'n18450': [71.11720873794593, 40.34374783090754],\n",
    " 'a01000': [1165.541519079269, 377.55451381280966],\n",
    " 'n00900': [115.89933209714266, 93.53687874198675],\n",
    " 'n59720': [195.99087061562665, 88.38279455455971],\n",
    " 'a04470': [4911.01605729394, 3764.85960547329],\n",
    " 'a18500': [886.9417040371746, 556.7414490923978],\n",
    " 'a02300': [484.1049012518553, 327.6492302587705]}\n",
    "values = df2.values\n",
    "\n",
    "columns = df2.columns\n",
    "for i in range(len(columns)):\n",
    "    normalizaiton = asd[columns[i]]\n",
    "    for j in range(values.shape[0]):\n",
    "        values[j][i] = (values[j][i] - normalizaiton[1]) / normalizaiton[0]\n",
    "\n",
    "print(values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ff8a847",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 43)\n",
      "Num training data:  43\n",
      "Num test data:  10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 128\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.16\n",
    "\n",
    "# convert data to torch.FloatTensor\n",
    "#transform = transforms.ToTensor()\n",
    "\n",
    "train_data = df.values\n",
    "test_data = values\n",
    "\n",
    "print(train_data.shape)\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx[:len(valid_idx)//2])\n",
    "# prepare data loaders (combine dataset and sampler)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "    sampler=valid_sampler, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "classes = [0,1,2,3,4,5,6,7,8,9]\n",
    "print('Num training data: ', len(train_data[0]))\n",
    "print('Num test data: ', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e80e93ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "090c5724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0683, -0.0714,  0.0295,  ..., -0.4533, -0.2111,  0.0000],\n",
      "        [-0.7574, -0.8607, -0.7887,  ..., -0.4533, -0.8287,  2.0000],\n",
      "        [-0.1644,  0.2468,  0.0332,  ..., -0.4533,  0.5829,  4.0000],\n",
      "        ...,\n",
      "        [ 4.6777,  4.2944,  4.4744,  ...,  2.7164,  3.6709,  6.0000],\n",
      "        [-0.3941, -0.3920, -0.4504,  ..., -0.4533, -0.5111,  1.0000],\n",
      "        [-0.6734, -0.7324, -0.6946,  ..., -0.4533, -0.8287,  1.0000]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57c75c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 43])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a0c3e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "def get_optimizer_scratch(model):\n",
    "    ## TODO: select and return an optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7db9237c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (fc1): Linear(in_features=42, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc5): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier , self).__init__()\n",
    "        self.fc1 = nn.Linear(42, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.fc5 = nn.Linear(64, 10)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        # Now with dropout\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "        x = self.dropout(F.relu(self.fc4(x)))\n",
    "\n",
    "        # output so no dropout here\n",
    "        x = F.log_softmax(self.fc5(x), dim=1)\n",
    "\n",
    "        return x\n",
    "Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "352fe6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "#model = Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ee372de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier(\n",
      "  (fc1): Linear(in_features=42, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc5): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Epoch: 0 Training Loss: 1.881504 Validation Loss: 41.133806\n",
      "Validation loss decreased (inf --> 41.133806).  Saving model ...\n",
      "Epoch: 1 Training Loss: 1.114277 Validation Loss: 25.374925\n",
      "Validation loss decreased (41.133806 --> 25.374925).  Saving model ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-1ced8de9057d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    105\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'step'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m             F.adam(params_with_grad,\n\u001b[0m\u001b[0;32m    108\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\torch\\optim\\_functional.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model = Classifier()\n",
    "model = model.cuda()\n",
    "print(model)\n",
    "optimizer = get_optimizer_scratch(model)\n",
    "save_path = ''\n",
    "# TODO: Train the network here\n",
    "epochs = 5000\n",
    "valid_loss_min = np.inf\n",
    "for e in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    for x in train_loader:\n",
    "#         print(\"here\")\n",
    "        if use_cuda:\n",
    "           x = x.cuda()\n",
    "        train = x[: , :-1]\n",
    "        labels = x[: , -1]\n",
    "        log_ps = model(train.float())\n",
    "        loss = criterion(log_ps, labels.long())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "#         print(f\"Tra>?ining loss: {running_loss}\")\n",
    "        pass\n",
    "    model.eval()\n",
    "    running_loss = running_loss / len(train_loader)\n",
    "    for x in valid_loader:\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "        train = x[: , :-1]\n",
    "        labels = x[: , -1]\n",
    "        log_ps = model(train.float())\n",
    "        loss = criterion(log_ps, labels.long())\n",
    "        \n",
    "        valid_loss += loss.item()\n",
    "#         print(valid_loss)\n",
    "    else:\n",
    "        pass\n",
    "#         print(f\"Validation loss: {valid_loss}\")\n",
    "    valid_losss =  valid_loss / len(valid_loader)\n",
    "    print('Epoch: {} Training Loss: {:.6f} Validation Loss: {:.6f}'.format(\n",
    "            e, \n",
    "            running_loss,\n",
    "            valid_loss\n",
    "            ))\n",
    "\n",
    "    ## TODO: if the validation loss has decreased, save the model at the filepath stored in save_path\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))\n",
    "        torch.save(model.state_dict(), 'E:\\UN hackathon\\AI tax evasion model\\model.pth')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bbcf5c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Classifier()\n",
    "model.load_state_dict(torch.load('E:\\UN hackathon\\AI tax evasion model\\model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48c965cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6],\n",
      "        [6],\n",
      "        [6],\n",
      "        [6],\n",
      "        [5],\n",
      "        [6],\n",
      "        [6],\n",
      "        [6],\n",
      "        [9],\n",
      "        [3]])\n"
     ]
    }
   ],
   "source": [
    "    # set the module to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    for x in test_loader:\n",
    "        # move to GPU\n",
    "        if use_cuda:\n",
    "#             x = x.cuda()\n",
    "            pass\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        test = x[: , :]\n",
    "        log_ps = model(test.float())\n",
    "        pred = log_ps.data.max(1, keepdim=True)[1]\n",
    "        print(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f0ae15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
